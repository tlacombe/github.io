{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE204: Lab exam session, 03/13/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: Theo Lacombe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the following carefully before starting the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab exam is composed of 3 exercises, each of them granting up to 7 points (the final grade is over 20, so there is one bonus point). \n",
    "\n",
    "These exercises are independant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules and other remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, standard exam rules hold for this lab session, that is no chatting, no use of communication tools, no use of smartphone, etc.\n",
    "\n",
    "As this is a exam on computer, some details must be specified:\n",
    "- You are allowed to call the responsible for **technical** (i.e. Python related) help in order to understand an error message if needed (that does not mean that we will debug your code).\n",
    "- Your answers to written questions (like \"What do you observe?\") must be **short**, basically one sentence at most (most of the time, a single word would fit perfectly).\n",
    "- You are allowed to use all standard Python tools along with the `numpy` library. However, you are **not** allowed to use `scikit-learn` except when it is explicitely mentionned in the question. (Basically, it means that if the question is \"implement a linear regression\", you can't just import the one of `scikit-learn`.)\n",
    "- <!-- add rules about documents -->\n",
    "- You won't be evaluated on the quality of your code, only on its correctness.\n",
    "- You must not change the name of the functions pre-written.\n",
    "- Beware of the cells you run (and the order you run them): don't run train the algorithm of exercise 3 on the data of exercise 2 by mistake!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# a side file that contains some useful test function & cie.\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 : $k$-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly recall the $k$ nearest neighbors ($k$-NN) algorithm seen in lab session 2, which is a classification method.\n",
    "\n",
    "Consider a dataset $\\mathcal{D} = (X, y)$ where the $k$-th coordinate of the $i$-th observation is denoted by $x^{(i)}_k$ and the corresponding label is $y^{(i)}$, or equilvalently:\n",
    "$$\\boldsymbol{X} = \\begin{pmatrix} x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\dots & \\vdots \\\\ x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.$$\n",
    "\n",
    "Fix an integer $k$. Given a new observation $x \\in \\mathbb{R}^p$, we produce an estimation of its class $\\hat{y}$ in the following way:\n",
    "\n",
    "Let $x^{(i_1)}, \\dots, x^{(i_k)}$ be the $k$ points in $X$ that minimizes $\\|x - x^{(i)}\\|_2$. Then $\\hat{y}$ is the predominent occurence in $\\{ y^{(i_1)}, \\dots, y^{(i_k)} \\}$. In case of equality between two (or more) classes, $\\hat{y}$ is randomly chosen (uniformly) among these classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** What is the training error achieved by this algorithm in the case $k=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Is the $k$-NN algorithm a _parametric_ or _non-parametric_ model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement a function `oneNN` that takes as an input a matrix `X_train` of shape `n x p`, corresponding labels `y_train` (shape `n`) a new observation `x` of shape `p`, and that returns an estimation of `hat_y` for this new observation using the **one**-nearest neighbor algorithm.\n",
    "\n",
    "_Python hint:_ You can compute the distance between two numpy arrays `x_i` and `x` using `np.linalg.norm(x_i, x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneNN(X_train, y_train, x):\n",
    "    \"\"\"\n",
    "        param X_train: np.array, shape N x p\n",
    "        param y_train: np.array, shape N\n",
    "        param x:       np.array, shape p\n",
    "        return: hat_y, class prediction.\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneNN(oneNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset of $n = 500$ points, with $3$ labels \"red\", \"blue\", and \"green\", encoded respectively by $0,1,2$ in the following. This dataset is splitted in a training set `x_train, y_train` with $400$ observations, and a test set `x_test, y_test` of $100$ data ; this dataset must be loaded by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_kNN_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** In this question, we allow the use of the `scikit-learn` library. \n",
    "\n",
    "**4.a.** Using it, perform $k$-NN classification for all $k = 1 \\dots 10$ by training on (`x_train`, `y_train`), and use it to predict labels for `x_test`. Evaluate the quality of the classification using the `accuracy_score` function provided by `scikit-learn`.\n",
    "\n",
    "\n",
    "We recall the use of `scikit-learn` to perform $k$-NN classification:\n",
    "- `clf = KNeighborsClassifier(n_neighbors = k)` creates an object of the class `classifier` that can be used to perform `n_neighbors`-NN classification (of course, you have to precise the value of `k` when instanciating the classifier).\n",
    "- `clf.fit(x_train,y_train)` trains this classifier on a training set (`x_train` is the observation matrix, `y_train` the labels).\n",
    "- `clf.predict(x_test)` returns label estimations for a new set of observations encoded as a matrix `x_test`.\n",
    "- Given `y_true` and `y_pred` which are respectively the true labels of the test set (`y_test` here) and the label predicted by the $k$-NN classifier, you can compute the accuracy score of the classification using `accuracy_score(y_true, y_pred)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.b.** Which value of $k$ reaches the best accuracy on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 : Linear regression and its extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly recall the framework of linear regression.\n",
    "\n",
    "Consider observations $x = x^{(1)} \\dots x^{(n)}$, with $x^{(i)} \\in \\mathbb{R}^p$, and labels $y^{(1)} \\dots y^{(n)}$.\n",
    "Given an observation $x^{(i)}$, and a vector $\\theta = (\\theta_0 \\dots \\theta_p)^T$, we produce an estimation $\\hat{y}^{(i)}$ of $y^{(i)}$ of the following form:\n",
    "$$ \\hat{y}^{(i)} = \\theta_0 + \\theta_1 x^{(i)}_1 + \\dots + \\theta_p x^{(i)}_p.$$\n",
    "We want to find an optimal vector $\\theta^*$ so that the average error made by using $\\hat{y}^{(i)}$ to approximate $y^{(i)}$ is small.\n",
    "With vector notations, it reads:\n",
    "$$\\theta^* \\in \\mathrm{argmin} (E(\\theta)), \\quad \\text{ where } \\quad E(\\theta) := \\frac{1}{n} \\| y - X \\cdot \\theta\\|_2, $$\n",
    "where \n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.\n",
    "$$\n",
    "We recall that a solution in closed form exists, namely the optimal $\\theta^*$ is given by:\n",
    "$$ \\theta^* = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Implement a function `linreg` that, given `X` and `y`, returns the vector optimal `theta`.\n",
    "\n",
    "_Python hints_ : \n",
    "- You can use `np.linalg.inv(A)` to compute the inverse of a (non-singular) **square** matrix. \n",
    "- You can use `np.transpose(A)` or equivalently `A.T` to compute the transpose of a numpy array `A`. \n",
    "- You can use `np.dot(A,B)`, or equivalently `A.dot(B)` to compute the matrix-matrix (or matrix-vector) $A \\cdot B$ product between two numpy arrays `A` and `B`. Beware of the shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,y):\n",
    "    \"\"\"\n",
    "        param X: np.array of shape N x (p+1)\n",
    "        param y: np.array of shape N\n",
    "        return:  np.array of shape (p+1)\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Implement a function `E` that, given `X,y,theta` computes the error $E(\\theta)$.\n",
    "\n",
    "_Python hint:_ You can compute the distance between two vector `hat_y` and `y` using `np.linalg.norm(hat_y,y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(X,y,theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float, evaluation of the error function.\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement a function `pred` that, given an observation `x` of shape `p` and a vector `theta`, returns a predicted value `hat_y`.\n",
    "\n",
    "_Hint:_ Do not forget the constant term $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x,theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape (p+1)\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your functions with the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_linear_regression(linreg, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the specific case where the observations $x^{(i)}$ are real-valued (that is $p=1$). For an integer $k$ fixed, and a vector $\\theta = (\\theta_0 \\dots \\theta_k)^T$, we propose to estimate $y^{(i)}$ in the following way:\n",
    "$$ \\hat{y}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\theta_2 (x^{(i)})^2 + \\dots + \\theta_k (x^{(i)})^k. $$\n",
    "Here, $(x^{(i)})^j$ means the $j$-th power of $x^{(i)}$.\n",
    "\n",
    "As in standard linear regression, the goal is to find a $\\theta^*$ so that \n",
    "$$ \\sum_{i=1}^n (y^{(i)} - \\hat{y}^{(i)})^2 $$\n",
    "is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Write a function `polynomial_expend` that, given a **real valued** vector $x \\in \\mathbb{R}^n$ and an integer $k$, returns the $n \\times (k+1)$ matrix\n",
    "$$X = \\begin{pmatrix} 1 & x^{(1)} & \\dots & (x^{(1)})^k \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x^{(n)} & \\dots & (x^{(n)})^k \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_expend(x,k):\n",
    "    \"\"\"\n",
    "        param x: np.array of shape N\n",
    "        param k: integer\n",
    "        return:  np.array of shape N x (p+1)\n",
    "    \"\"\"\n",
    "    return None #<--to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset of $n = 50$ couple of (observations, labels), that is splitted into a _train set_ of size $40$ and a _test set_ of size $10$. These data are loaded using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data_polyreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Using the functions that you have written above, \n",
    "\n",
    "**5.a.** Perform a polynomial regression to fit `x_train, y_train` ; with $k = 1 \\dots 12$ and then predict labels for for the test set `x_test`. \n",
    "\n",
    "The test error is defined as\n",
    "$$ E_\\mathrm{test} (\\hat{y}, y_\\mathrm{test}) = \\frac{1}{n_\\mathrm{test}} \\|\\hat{y} - y_\\mathrm{test}\\|, $$\n",
    "where $n_\\mathrm{test}$ is the number of observation in the test set.\n",
    "\n",
    "Store training errors and test errors you obtain as two `np.array`s of size $10$, whose names must be (respectively) `train_errors` and `test_errors`, and such that `train_errors[k]` (resp. `test_errors[k]`) gives the training error (resp. test error) obtained by performing polynomial regression with maximal degree $k$.\n",
    "\n",
    "_Python hint:_ Recall that you can compute the distance between two numpy array using `np.linalg.norm(hat_y, y_test)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write some code here to obtain the two list `train_scores` and `test_scores`.\n",
    "\n",
    "#...\n",
    "\n",
    "#train_errors = ... #<--to uncomment and to complete\n",
    "#test_errors = ... #<--to uncomment and to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell (beware of variable names!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linreg_errors(train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.b.** What is the name of the phenomenon we are observing here? (short answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write something here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is a common model used to perform binary classification that is however formulated as a regression problem. \n",
    "\n",
    "We have a *learning set* $\\mathcal{D} = \\{\\boldsymbol{x^{(i)}}, y^{(i)}\\}_{1 \\leq i \\leq n}$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "We consider the logistic function:\n",
    "$$ \\sigma : t \\mapsto \\frac{1}{1 + e^{-t}}.$$\n",
    "Note that $\\sigma$ takes values in $(0,1)$ and will be used to estimate $y^{(i)}$ given $x^{(i)}$. More precisely, given a weight vector $\\theta = ( \\theta_0 \\dots \\theta_p )^T$ and a vector $X^{(i)} = (1, x^{(i)}_1 \\dots x^{(i)}_p)$, we make the following estimation:\n",
    "$$\n",
    "    \\hat{y}^{(i)} = \\begin{cases} 1 \\text{ if } \\sigma(X^{(i)} \\theta) > 1/2 \\\\\n",
    "                                  0 \\text{ otherwise } \n",
    "                    \\end{cases}\n",
    "$$\n",
    "\n",
    "We want to find a good $\\theta$. One can show that the optimal $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ minimizes the following optimization problem:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* \\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) \\quad \\text{with} \\quad E(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log ( 1 + \\exp( - y^{(i)} X^{(i)} \\cdot \\theta ) )$$\n",
    "with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : Does the error function $E$ have a unique minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** \n",
    "\n",
    "**2.a.** Implement a function `sigma` that compute, given $x^{(i)} \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$ the quantity $\\sigma(X^{(i)} \\cdot \\theta)$, where $X^{(i)} = (1, x^{(i)}_1, \\dots, x^{(i)}_p)$.\n",
    "\n",
    "_Python hint:_ `theta[1:]` returns the vector $(\\theta_1 \\dots \\theta_p)$, while `theta[0]` returns $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x, theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float.\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** Implement a function `pred` that provides, given $x^{(i)} \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$, an estimate $\\hat{y}^{(i)}$ of $y^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape p+1\n",
    "        return: integer, 0 or 1.\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement the error function we want to minimize $E$, that takes $x, y, \\theta$ as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(X, y, theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no closed form for the optimal $\\theta^*$, we will minimize $E$ using a gradient descent. After computing (on a sheet of paper, that you will keep for yourself) the gradient of $E$ theoretically, implement a function `grad_E(X,y,theta)` that returns the gradient (with respect to $\\theta$) of $E$ for the current estimation $\\theta$ of $E$.\n",
    "\n",
    "You can test your implementation with following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_E(X, y, theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: np.array of shape (p+1)\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grad(grad_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Complete the following code in order to implement the gradient descent algorithm to minimize $E$ that returns the final estimation of $\\theta$ and the evolution of energy over time. The hyper parameters (number of steps, learning rates) are set by default and should not be changed.\n",
    "\n",
    "Note also that we supposed we are only given $x$, not $X$ so you should reconstruct it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(x, y, eta=0.001, nb_max_step=10000, stopping_criterion=0.01):\n",
    "    \"\"\"\n",
    "        param x:                  np.array, input observations, shape N x p\n",
    "        param y:                  np.array, input labels, shape N (filled with 0 and 1)\n",
    "        param eta:                float, learning rate.\n",
    "        param nb_max_step:        int, maximal number of steps done in the gradient descent.\n",
    "        param stopping_criterion: float, stopping criterion on gradient process.\n",
    "        \n",
    "        return: np.array of shape (p+1) (theta), list of float (evolution_of_loss)\n",
    "        \n",
    "        Remark: do not change the default values!\n",
    "    \"\"\"\n",
    "    # Storing some useful values\n",
    "    N, p = x.shape\n",
    "    # List to store the evolution of E(theta) over steps\n",
    "    evolution_of_loss = []\n",
    "    # Store the current value of loss to stop gradient descent\n",
    "    e = np.inf\n",
    "    # Initialization of theta\n",
    "    np.random.seed(seed=42)  # DO NOT change this\n",
    "    theta = np.random.rand(p+1)\n",
    "    \n",
    "    ### Transform x into X\n",
    "    X = add_ones_left(x)\n",
    "    \n",
    "    ### Perform the gradient descent\n",
    "    for t in range(nb_max_step):\n",
    "        \n",
    "        # Perform a gradient step\n",
    "        #<-- to complete\n",
    "        new_e = 0 #<-- to complete, store the loss according to the new parameter theta\n",
    "        \n",
    "        # We stop the gradient descent if the variation in the loss function is small\n",
    "        if np.abs(new_e - e) < stopping_criterion:\n",
    "            break  # break allow to exit the for loop\n",
    "        # otherwise, we just set e = new_e and store it\n",
    "        else:\n",
    "            e = new_e\n",
    "            evolution_of_loss.append(e)\n",
    "    \n",
    "    return theta, evolution_of_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a 1D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** test your code with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1d_logreg(grad_descent, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a 2D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset composed of a training set `(x_train,y_train)` with $N = 280$ points and a test set `x_test` of $N' = 20$ points (along with some unknown labels `y_test`) that you can load using the following cell. The score of a classifier is defined as\n",
    "$$ \\frac{1}{N'} \\sum_{i=1}^{N'} \\delta_{\\mathrm{pred}(x_\\mathrm{test}^{(i)}), y_\\mathrm{test}^{(i)}} $$\n",
    "where\n",
    "$$\\delta_{\\alpha,\\beta} = \\begin{cases} 1 \\text{ if } \\alpha = \\beta \\\\ 0 \\text{ otherwise } \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_2D_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** \n",
    "\n",
    "**6.a.** What is the training error of the logistic regression on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer of the question in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.b.** What is your test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer of the question in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
