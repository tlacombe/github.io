{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE204: Lab exam session, 03/13/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: Theo Lacombe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the file you will have to upload on Moodle at (or before) 15:15 (for group 1) and 17:30 (for group 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the following carefully before starting the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8 :: Anaconda, Inc.\n",
      "must be Python3 or more\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"must be Python3 or more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For users on Google collab (skip this if you run on a local notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF AND ONLY IF you are using Google Collab, uncomment the following cells and\n",
    "\n",
    "Run the two following cells, click on Browse and upload files \n",
    "\n",
    "- utils.py\n",
    "- data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab exam is composed of 3 exercises, each of them granting up to 7 points (the final grade is over 20, so there is one bonus point).\n",
    "\n",
    "These exercises are independant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules and other remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, standard exam rules hold for this lab session, that is no chatting, no use of communication tools, no use of smartphone, etc.\n",
    "\n",
    "As this is a exam on computer, some details must be specified:\n",
    "- You are allowed to call the responsible for **technical** (i.e. Python related) help in order to understand an error message if needed (that does not mean that we will debug your code).\n",
    "- Your answers to written questions (like \"What do you observe?\") must be **short**, basically one sentence at most (most of the time, a single word would fit perfectly).\n",
    "- You are allowed to use all standard Python tools along with the `numpy` library. However, you are **not** allowed to use `scikit-learn` except when it is explicitely mentionned in the question. (Basically, it means that if the question is \"implement a linear regression\", you can't just import the one of `scikit-learn`.)\n",
    "- Use `np.array`, **not** `np.matrix`.\n",
    "- You are not allowed to use any complementary document.\n",
    "- You won't be evaluated on the quality of your code, only on its correctness.\n",
    "- You must not change the name of the functions pre-written.\n",
    "- Beware of the cells you run (and the order you run them): don't train the algorithm of exercise 3 on the data of exercise 2 by mistake!\n",
    "- Despite our best efforts, some typo might remain. If you think you have noticed one, call us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# a side file that contains some useful test functions & cie.\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 : $k$-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly recall the $k$ nearest neighbors ($k$-NN) algorithm seen in lab session 2, which is used as a classification method here.\n",
    "\n",
    "Consider a dataset $\\mathcal{D} = (X, y)$ where the $k$-th coordinate of the $i$-th observation is denoted by $x^{(i)}_k$ and the corresponding label is $y^{(i)}$, or equilvalently:\n",
    "$$\\boldsymbol{X} = \\begin{pmatrix} x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\dots & \\vdots \\\\ x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.$$\n",
    "\n",
    "Assume a fixed integer $k < n$. Given a new observation $x \\in \\mathbb{R}^p$, we produce an estimation of its class $\\hat{y}$ in the following way:\n",
    "\n",
    "Let $x^{(i_1)}, \\dots, x^{(i_k)}$ be the $k$ nearest neighbors of $x$ in $X$ (i.e. the points in $X$ that achieve the $k$ lowest values of $\\|x - x^{(i)}\\|_2$). Then $\\hat{y}$ is the predominent occurence in $\\{ y^{(i_1)}, \\dots, y^{(i_k)} \\}$. In case of equality between two (or more) classes, $\\hat{y}$ is randomly chosen (uniformly) among these classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** What is the training error achieved by this algorithm in the case $k=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Is the $k$-NN algorithm a _parametric_ or _non-parametric_ model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement a function `oneNN` that takes as an input a matrix `X_train` of shape `n x p`, corresponding labels `y_train` (shape `n`) a new observation `x` of shape `p`, and that returns an estimation of `hat_y` for this new observation using the **one**-nearest neighbor algorithm.\n",
    "\n",
    "_Python hint:_ You can compute the distance in norm-2 $\\|x_i - x\\|_2$ between two numpy arrays `x_i` and `x` using `np.linalg.norm(x_i - x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneNN(X_train, y_train, x):\n",
    "    \"\"\"\n",
    "        param X_train: np.array, shape N x p\n",
    "        param y_train: np.array, shape N\n",
    "        param x:       np.array, shape p\n",
    "        return: float, hat_y, class prediction.\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running the following cell. Running time should be about few seconds. \n",
    "Note that the grid used to plot classification boundaries is intentionally coarse (to avoid running time issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_oneNN(oneNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset of $n = 700$ points, with $3$ labels \"blue\", \"red\", and \"green\", encoded respectively by $0,1,2$ in the following. This dataset is split in a training set `x_train, y_train` with $300$ observations, and a test set `x_test, y_test` of $400$ points ; this dataset must be loaded by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_kNN_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** In this question, we allow the use of the `scikit-learn` library. \n",
    "\n",
    "**4.a.** Using `scikit-learn`, perform $k$-NN classification for all $k = 1 \\dots 10$ by training on (`x_train`, `y_train`), and use it to predict labels for `x_test`. Evaluate the quality of the classification using the `accuracy_score` function provided by `scikit-learn`.\n",
    "\n",
    "\n",
    "We recall the use of `scikit-learn` to perform $k$-NN classification:\n",
    "- `clf = KNeighborsClassifier(n_neighbors = k)` creates an object of the class `classifier` that can be used to perform `n_neighbors`-NN classification (of course, you have to precise the value of `k` when instanciating the classifier).\n",
    "- `clf.fit(x_train,y_train)` trains this classifier on a training set (`x_train` is the observation matrix, `y_train` the labels).\n",
    "- `clf.predict(x_test)` returns label estimations for a new set of observations encoded as a matrix `x_test`.\n",
    "- Given `y_test` and `y_pred` which are respectively the true labels of the test set and the label predicted by the $k$-NN classifier, you can compute the accuracy score (in %) of the classification using `score(y_test, y_pred)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.b.** Which value of $k$ reaches the best accuracy on the test set? For which accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write your answer here-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 : Linear regression and its extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly recall the framework of linear regression.\n",
    "\n",
    "Consider observations $x = x^{(1)} \\dots x^{(n)}$, with $x^{(i)} \\in \\mathbb{R}^p$, and labels $y^{(1)} \\dots y^{(n)}$.\n",
    "Given an observation $x^{(i)}$, and a vector $\\theta = (\\theta_0 \\dots \\theta_p)^T$, we produce an estimation $\\hat{y}^{(i)}$ of $y^{(i)}$ of the following form:\n",
    "$$ \\hat{y}^{(i)} = \\theta_0 + \\theta_1 x^{(i)}_1 + \\dots + \\theta_p x^{(i)}_p = \\theta_0 + x^{(i)} \\cdot (\\theta_1 , \\dots , \\theta_p)^T.$$\n",
    "We want to find an optimal vector $\\theta^*$ so that the average error made by using $\\hat{y}^{(i)}$ to approximate $y^{(i)}$ is small.\n",
    "With vector notations, it reads:\n",
    "$$\\theta^* \\in \\mathrm{argmin} (E(\\theta)), \\quad \\text{ where } \\quad E(\\theta) := \\frac{1}{n} \\| y - X \\cdot \\theta\\|_2, $$\n",
    "where \n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}.\n",
    "$$\n",
    "We recall that a solution in closed form exists, namely the optimal $\\theta^*$ is given by:\n",
    "$$ \\theta^* = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Some Python hints useful in this exercise:\n",
    "_Python hints_ : \n",
    "- You can use `np.linalg.inv(A)` to compute the inverse of a (non-singular) **square** matrix. \n",
    "- You can use `np.transpose(A)` or equivalently `A.T` to compute the transpose of a numpy array `A`. \n",
    "- You can use `np.dot(A,B)`, or equivalently `A.dot(B)` to compute the matrix-matrix (or matrix-vector) $A \\cdot B$ product between two numpy arrays `A` and `B`. Beware of the shapes!\n",
    "- Given a 1D np.array (encoding a vector) `t=[t_0 ... t_(n-1)]`, using `t[i:]` will return `[t_i, ... , t_(n-1)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Implement a function `linreg` that, given `X` and `y`, returns the vector optimal `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linreg(X,y):\n",
    "    \"\"\"\n",
    "        param X: np.array of shape N x (p+1)\n",
    "        param y: np.array of shape N\n",
    "        return:  np.array of shape (p+1)\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Implement a function `E` that, given `X,y,theta` computes the error $E(\\theta)$.\n",
    "\n",
    "_Hint:_ Do not forget the $\\frac{1}{n}$ factor.\n",
    "\n",
    "_Python hint:_ You can compute the distance in norm-2 $\\| \\cdot \\|_2$ between two vector `hat_y` and `y` using `np.linalg.norm(hat_y - y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E(X,y,theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float, evaluation of the error function.\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement a function `pred` that, given an observation `x` of shape `p` and a vector `theta`, returns a predicted value `hat_y`.\n",
    "\n",
    "_Hint:_ Do not forget the constant term $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(x,theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float, the prediction hat_y.\n",
    "    \"\"\"\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your functions with the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_linear_regression(linreg, E, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the specific case where the observations $x^{(i)}$ are real-valued (that is $p=1$). For an integer $k$ fixed, and a vector $\\theta = (\\theta_0 \\dots \\theta_k)^T$, we propose to estimate $y^{(i)}$ in the following way:\n",
    "$$ \\hat{y}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} + \\theta_2 (x^{(i)})^2 + \\dots + \\theta_k (x^{(i)})^k. $$\n",
    "Here, $(x^{(i)})^j$ means the $j$-th power of $x^{(i)}$.\n",
    "\n",
    "As in standard linear regression, the goal is to find a $\\theta^*$ so that \n",
    "$$ \\frac{1}{n} \\|y - \\hat{y}\\|_2 $$\n",
    "is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Write a function `polynomial_expend` that, given a **real valued** vector $x \\in \\mathbb{R}^n$ and an integer $k$, returns the $n \\times (k+1)$ matrix\n",
    "$$X = \\begin{pmatrix} 1 & x^{(1)} & \\dots & (x^{(1)})^k \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x^{(n)} & \\dots & (x^{(n)})^k \\end{pmatrix}$$\n",
    "\n",
    "_Python hint_: Given a vector `x`, `x**k` returns the vector whose coordinates are the $k$-th power of those in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_expend(x,k):\n",
    "    \"\"\"\n",
    "        param x: np.array of shape N\n",
    "        param k: integer\n",
    "        return:  np.array of shape N x (k+1)\n",
    "    \"\"\"\n",
    "    return None #<--to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset of $n = 100$ couple of (observations, labels), that is split into a _train set_ of size $50$ and a _test set_ of size $50$. These data are loaded using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data_polyreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Using the functions that you have written above, \n",
    "\n",
    "**5.a.** Perform a polynomial regression to fit `x_train, y_train` ; with $k = 1 \\dots 12$ and predict labels for the test set `x_test`. \n",
    "\n",
    "The test error is defined as\n",
    "$$ E_\\mathrm{test} (\\hat{y}, y_\\mathrm{test}) = \\frac{1}{n_\\mathrm{test}} \\|\\hat{y} - y_\\mathrm{test}\\|_2, $$\n",
    "where $n_\\mathrm{test}$ is the number of observation in the test set.\n",
    "\n",
    "Store training errors and test errors you obtain as two `np.array`s of size $12$, whose names must be (respectively) `train_errors` and `test_errors`, and such that `train_errors[k]` (resp. `test_errors[k]`) gives the training error (resp. test error) obtained by performing polynomial regression with degree $k$.\n",
    "\n",
    "_Python hint:_ Recall that you can compute the norm-2 $\\| \\cdot \\|$ of a numpy array `t` using `np.linalg.norm(t)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write some code here to obtain the two list `train_errors` and `test_errors`.\n",
    "\n",
    "#...\n",
    "\n",
    "#train_errors = ... #<--to uncomment and to complete\n",
    "#test_errors = ... #<--to uncomment and to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell (beware of variable names!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_linreg_errors(train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.b.** What is the name of the phenomenon we are observing here? (short answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- write something here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression is a common model used to perform binary classification that is however formulated as a regression problem. \n",
    "\n",
    "We have a *learning set* $\\mathcal{D} = \\{{x^{(i)}}, y^{(i)}\\}_{1 \\leq i \\leq n}$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "We consider the _sigmoid_ (logistic) function:\n",
    "$$ \\sigma : t \\mapsto \\frac{1}{1 + e^{-t}}.$$\n",
    "Note that $\\sigma$ takes values in $(0,1)$ and will be used to estimate $y^{(i)}$ given $x^{(i)}$. More precisely, given a weight vector $\\theta = ( \\theta_0 \\dots \\theta_p )^T$ and a vector $X^{(i)} = (1, x^{(i)}_1 \\dots x^{(i)}_p)$, we make the following estimation:\n",
    "$$\n",
    "    \\hat{y}^{(i)} = \\begin{cases} 1 \\text{ if } \\sigma(X^{(i)} \\theta) > 1/2 \\\\\n",
    "                                  0 \\text{ otherwise } \n",
    "                    \\end{cases}\n",
    "$$\n",
    "\n",
    "We want to find a good $\\theta$. One can show that the optimal $\\boldsymbol{\\theta}^* = \\begin{pmatrix} \\theta_0^* & \\dots & \\theta_p^* \\end{pmatrix}^T$ minimizes the following optimization problem:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* \\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$$\n",
    "where\n",
    "$$E(\\boldsymbol{\\theta}) = - \\frac{1}{n} \\sum_{i=1}^n \\log(\\sigma(X^{(i)} \\cdot \\theta)) y^{(i)} + \\log(1 - \\sigma(X^{(i)} \\cdot \\theta)) (1 - y^{(i)})$$\n",
    "with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** : Does the error function $E$ have a unique minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** \n",
    "\n",
    "**2.a.** Implement a function `my_dot` that computes, given $x^{(i)} \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$ the quantity $X^{(i)} \\cdot \\theta = \\theta_0 + x^{(i)} \\cdot (\\theta_1 , \\dots , \\theta_p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_dot(x,theta):\n",
    "    return None #<-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.b.** Implement a function `sigmoid` that computes, given given $x^{(i)} \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$, the quantity $\\sigma(X^{(i)} \\cdot \\theta)$.\n",
    "\n",
    "_Python hint:_ you can use `np.exp(x)` to compute the exponentiation of a float (or term-wise exponentiation of an array) `x`. Similarly, you can use `np.log` to compute the corresponding logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float.\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.c.** Implement a function `pred` that provides, given $x^{(i)} \\in \\mathbb{R}^p$ and $\\theta \\in \\mathbb{R}^{p+1}$, an estimate $\\hat{y}^{(i)}$ of $y^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(x, theta):\n",
    "    \"\"\"\n",
    "        param x:     np.array of shape p\n",
    "        param theta: np.array of shape p+1\n",
    "        return: integer (or float), 0 or 1.\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Implement the error function we want to minimize $E$, that takes $x, y, \\theta$ as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E(X, y, theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: float\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no closed form for the optimal $\\theta^*$, we will minimize $E$ using a gradient descent. \n",
    "\n",
    "**Question 4:** Implement a function `grad_E(X,y,theta)` that returns the gradient (with respect to $\\theta$) of $E$ for the current estimation $\\theta$ of $E$.\n",
    "\n",
    "_Hints:_ If you don't remember the expression of the gradient for this problem seen in lectures 4 and 5, you can derive it on a separate sheet of paper. Do not forget the $\\frac{1}{n}$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_E(X, y, theta):\n",
    "    \"\"\"\n",
    "        param X:     np.array of shape N x (p+1)\n",
    "        param y:     np.array of shape N\n",
    "        param theta: np.array of shape (p+1)\n",
    "        return: np.array of shape (p+1)\n",
    "    \"\"\"\n",
    "    return None # <-- to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Complete the following code in order to implement the gradient descent algorithm to minimize $E$ that returns the final estimation of $\\theta$ and the evolution of energy over time. \n",
    "\n",
    "The hyper parameters (number of steps, learning rates) are set by default and should not be changed.\n",
    "\n",
    "The gradient descent will use the following rules:\n",
    "- If $E(\\theta^{(t+1)}) > E(\\theta^{(t)})$, then we devide the learning rate (stored in `eta`) by $2$.\n",
    "- If $\\left|E(\\theta^{(t+1)}) - E(\\theta^{(t)})\\right|$ is smaller than a fixed value (stored in `stopping_criterion`), we stop the algorithm.\n",
    "\n",
    "_Python hints:_ \n",
    "- You can use `np.abs(x)` to obtain the absolute value of a float `x`.\n",
    "\n",
    "- You can use `break` to exit a `for` loop. For example, \n",
    "\n",
    "        for t in range(1000): \n",
    "            print(t)\n",
    "            if t == 42:\n",
    "                break\n",
    "            \n",
    "will print `0, 1, ... , 42` (and nothing more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(x, y, eta=0.001, nb_max_step=10000, stopping_criterion=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "        param x:                  np.array, input observations, shape N x p\n",
    "        param y:                  np.array, input labels, shape N (filled with 0 and 1)\n",
    "        param eta:                float, learning rate.\n",
    "        param nb_max_step:        int, maximal number of steps done in the gradient descent.\n",
    "        param stopping_criterion: float, stopping criterion on gradient process.\n",
    "        \n",
    "        return: np.array of shape (p+1) (theta), list of float (evolution_of_loss)\n",
    "        \n",
    "        Remark: do not change the default values!\n",
    "    \"\"\"\n",
    "    # Storing some useful values\n",
    "    N, p = x.shape\n",
    "    # List to store the evolution of E(theta) over steps\n",
    "    evolution_of_loss = []\n",
    "    # Store the current value of loss to stop gradient descent\n",
    "    e = np.inf\n",
    "    # Initialization of theta (using some hidden function)\n",
    "    theta = theta_init(p)\n",
    "    \n",
    "    ### Perform the gradient descent\n",
    "    for t in range(nb_max_step):\n",
    "        \n",
    "        # Perform a gradient step\n",
    "        #<-- to complete\n",
    "        new_e = 0 #<-- to complete, store the loss according to the new parameter theta\n",
    "        \n",
    "        # In testing process (below) we print the evolution\n",
    "        # of the training error (which should decrease)\n",
    "        if verbose and ((t+1)%10)==0:\n",
    "            print(\"Training error after\",t+1,\"steps:\", np.round(new_e,5))\n",
    "        \n",
    "        # Stopping criterion:\n",
    "        #<--to complete\n",
    "    \n",
    "    return theta, evolution_of_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a 1D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** test your code with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1d_logreg(grad_descent, pred, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a 2D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset composed of a training set `(x_train,y_train)` with $N = 280$ points and a test set `x_test` of $N' = 20$ points (along with some unknown labels `y_test`) that you can load using the following cell. The score of a classifier is defined as\n",
    "$$ \\frac{1}{N'} \\sum_{i=1}^{N'} \\delta_{\\mathrm{pred}(x_\\mathrm{test}^{(i)}), y_\\mathrm{test}^{(i)}} $$\n",
    "where\n",
    "$$\\delta_{\\alpha,\\beta} = \\begin{cases} 1 \\text{ if } \\alpha = \\beta \\\\ 0 \\text{ otherwise } \\end{cases}$$\n",
    "and can be computed using the function `score(y_pred, y_test)` (return the result in %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_2D_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** \n",
    "\n",
    "**6.a.** What is the training error $E(\\theta^*)$ of the logistic regression on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer of the question in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.b.** What is your classification score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write some code here to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer of the question in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
